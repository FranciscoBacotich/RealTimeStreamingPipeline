## ğŸ““ BitÃ¡cora de Desarrollo (verificar lo marcado con âš ï¸)

### ğŸ”¹ Setup inicial fallido en Windows

* Intento fallido de usar Airflow con PowerShell debido a restricciones del sistema para ejecutar scripts (`ExecutionPolicy`).
* InstalaciÃ³n de Airflow fallÃ³ con error de launcher â†’ causado por rutas rotas al mover carpetas del entorno virtual âš ï¸.

### ğŸ”¹ DecisiÃ³n: migrar a WSL2

* Se instalÃ³ Ubuntu via WSL2.
* Problemas al crear `venv` por `ensurepip` ausente â†’ se solucionÃ³ con `sudo apt install python3.10-venv` âš ï¸.
* ActivaciÃ³n exitosa del entorno y prueba de Airflow en entorno Linux.

### ğŸ”¹ ComposiciÃ³n del entorno Docker

* Se definieron servicios en `docker-compose.yml`: Zookeeper, Kafka, Schema Registry, Control Center, Airflow (webserver y scheduler), PostgreSQL, Spark Master & Worker, Cassandra.
* Se aÃ±adiÃ³ un `entrypoint.sh` para inicializar correctamente el webserver de Airflow.
* Se agregÃ³ Cassandra y Spark con sus respectivas configuraciones internas âš ï¸.

### ğŸ”¹ ConexiÃ³n y pruebas de Kafka

* Se ejecutÃ³ comando para listar topics y crear el topic `users_created` si no existÃ­a.
* Se validÃ³ que Kafka estaba recibiendo datos desde Airflow usando Kafka CLI y Control Center.

### ğŸ”¹ Errores encontrados

* DAG no ejecutaba porque faltaba librerÃ­a `kafka-python` â†’ se agregÃ³ a `requirements.txt` âš ï¸.
* DAG no corrÃ­a porque `catchup` no estaba habilitado o mal configurado âš ï¸.
* Spark inicialmente sin entender del todo su rol â†’ se revisÃ³ e implementÃ³ manualmente âš ï¸.

### ğŸ”¹ Observaciones tÃ©cnicas

* El topic de Kafka no se define en YAML, sino que se crea manualmente o automÃ¡ticamente (dependiendo del `auto.create.topics.enable`).
* Los datos en Kafka viven en `/var/lib/kafka/data` pero no se gestionan directamente ahÃ­.
* Spark y Cassandra se comunican vÃ­a drivers (`pyspark`, `cassandra-driver`).
* Se usa `broker:29092` como direcciÃ³n del broker desde contenedores internos.

### ğŸ”¹ Mejoras futuras sugeridas âš ï¸

* Configurar Spark como proceso 24/7.
* Permitir creaciÃ³n segura de topics.
* Separar desarrollo de producciÃ³n vÃ­a volÃºmenes persistentes.
* Automatizar ejecuciÃ³n de `spark_stream.py` desde Airflow u otro scheduler.

---

Â¿Listo para escalar esto a producciÃ³n? ğŸš€

No, pero si.

