# Proyecto de Pipeline de Streaming con Apache Kafka, Airflow, Spark y Cassandra

Este proyecto consiste en un pipeline de procesamiento de datos en tiempo real utilizando Airflow para orquestaciÃ³n, Kafka para ingesta de datos en streaming, Spark para procesamiento distribuido y Cassandra como base de datos NoSQL para almacenamiento.

---

## Arquitectura General

```
API (randomuser.me) --> Airflow DAG --> Kafka Topic --> Spark Structured Streaming --> Cassandra
```

## Componentes Explicados

### ğŸ” Apache Airflow

Orquesta tareas y ejecuta DAGs (Directed Acyclic Graphs) definidos en Python. En este caso, el DAG se encarga de:

* Llamar a la API `randomuser.me`.
* Formatear la respuesta.
* Enviar los datos formateados a Kafka.

### ğŸ˜ PostgreSQL

Utilizado como base de datos interna para Airflow. No se utiliza para almacenar datos del pipeline.

### ğŸ³ Kafka + Zookeeper

Kafka maneja los mensajes en tiempo real (usuarios generados por la API). Zookeeper coordina los brokers de Kafka.

* **Broker**: Maneja los topics, productores y consumidores.
* **Schema Registry**: Registra los esquemas de los mensajes.
* **Control Center**: Interfaz visual para monitorear Kafka, topics, alertas, flujos de mensajes.

### âš¡ Apache Spark

Spark Structured Streaming actÃºa como consumidor de Kafka. Lee los mensajes del topic `users_created`, los transforma y los guarda en Cassandra.

* **Spark Master**: Nodo principal del cluster Spark.
* **Spark Worker**: Nodo que ejecuta las tareas distribuidas.

### ğŸ—„ï¸ Apache Cassandra

Base de datos NoSQL distribuida que almacena los usuarios generados por el sistema. Es ideal para escalabilidad horizontal y escritura rÃ¡pida.

---

## Requisitos del Proyecto

### requirements.txt

```txt
requests
kafka-python
pyspark==4.0.0
cassandra-driver==3.29.2
```

### Dockerfile personalizado para Airflow

```Dockerfile
FROM apache/airflow:2.6.0-python3.9
USER root
RUN apt-get update && \
    apt-get install -y openjdk-17-jdk && \
    apt-get clean
ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
ENV PATH="$JAVA_HOME/bin:$PATH"
USER airflow
```

---

## ğŸ§ª CÃ³mo probar el pipeline paso a paso

### 1. Activar integraciÃ³n WSL2 con Docker (si estÃ¡s en Windows)

* Docker Desktop â†’ Settings â†’ Resources â†’ WSL Integration â†’ Activar.

### 2. Reconstruir y levantar los contenedores

```bash
docker-compose down -v --remove-orphans
docker-compose build
docker-compose up -d
```

EsperÃ¡ que todos estÃ©n **healthy**.

### 3. Crear el topic en Kafka (si no existe)

```bash
docker exec -it broker kafka-topics --create \
  --bootstrap-server broker:29092 \
  --replication-factor 1 --partitions 1 --topic users_created
```

### 4. Ingresar al Airflow UI ([http://localhost:8080](http://localhost:8080))

* Activar el DAG `user_automation`
* Ejecutarlo manualmente o esperar la programaciÃ³n diaria.

### 5. Verificar Kafka

```bash
docker exec -it broker kafka-console-consumer \
  --bootstrap-server broker:29092 \
  --topic users_created --from-beginning
```

O visitar `http://localhost:9021` para usar el Control Center.

### 6. Ejecutar el script de Spark (por ahora manual)

```bash
source venv/bin/activate
python spark_stream.py
```

Este script:

* Conecta a Kafka.
* Lee desde el topic.
* Procesa los datos.
* Guarda en Cassandra.

### 7. Verificar los datos en Cassandra

```bash
docker exec -it cassandra cqlsh
> SELECT * FROM users_stream.users;
```

---

## ğŸ’¡ Diferencias con un pipeline de producciÃ³n

| Tu proyecto                   | ProducciÃ³n real                 |
| ----------------------------- | ------------------------------- |
| DAG con trigger manual        | DAG con disparadores continuos  |
| Spark manual                  | Spark streaming 24/7            |
| Cassandra local               | Cassandra en clÃºster multi-nodo |
| Kafka local y sin TLS         | Kafka seguro + monitoreo        |
| Sin almacenamiento intermedio | S3, HDFS, Lakehouse             |

---

## PrÃ³ximos pasos sugeridos

* Mas data.

---

## ğŸ““ BitÃ¡cora de Desarrollo (verificar lo marcado con âš ï¸)

### ğŸ”¹ Setup inicial fallido en Windows

* Intento fallido de usar Airflow con PowerShell debido a restricciones del sistema para ejecutar scripts (`ExecutionPolicy`).
* InstalaciÃ³n de Airflow fallÃ³ con error de launcher â†’ causado por rutas rotas al mover carpetas del entorno virtual âš ï¸.

### ğŸ”¹ DecisiÃ³n: migrar a WSL2

* Se instalÃ³ Ubuntu via WSL2.
* Problemas al crear `venv` por `ensurepip` ausente â†’ se solucionÃ³ con `sudo apt install python3.10-venv` âš ï¸.
* ActivaciÃ³n exitosa del entorno y prueba de Airflow en entorno Linux.

### ğŸ”¹ ComposiciÃ³n del entorno Docker

* Se definieron servicios en `docker-compose.yml`: Zookeeper, Kafka, Schema Registry, Control Center, Airflow (webserver y scheduler), PostgreSQL, Spark Master & Worker, Cassandra.
* Se aÃ±adiÃ³ un `entrypoint.sh` para inicializar correctamente el webserver de Airflow.
* Se agregÃ³ Cassandra y Spark con sus respectivas configuraciones internas âš ï¸.

### ğŸ”¹ ConexiÃ³n y pruebas de Kafka

* Se ejecutÃ³ comando para listar topics y crear el topic `users_created` si no existÃ­a.
* Se validÃ³ que Kafka estaba recibiendo datos desde Airflow usando Kafka CLI y Control Center.

### ğŸ”¹ Errores encontrados

* DAG no ejecutaba porque faltaba librerÃ­a `kafka-python` â†’ se agregÃ³ a `requirements.txt` âš ï¸.
* DAG no corrÃ­a porque `catchup` no estaba habilitado o mal configurado âš ï¸.
* Spark inicialmente sin entender del todo su rol â†’ se revisÃ³ e implementÃ³ manualmente âš ï¸.

### ğŸ”¹ Observaciones tÃ©cnicas

* El topic de Kafka no se define en YAML, sino que se crea manualmente o automÃ¡ticamente (dependiendo del `auto.create.topics.enable`).
* Los datos en Kafka viven en `/var/lib/kafka/data` pero no se gestionan directamente ahÃ­.
* Spark y Cassandra se comunican vÃ­a drivers (`pyspark`, `cassandra-driver`).
* Se usa `broker:29092` como direcciÃ³n del broker desde contenedores internos.

### ğŸ”¹ Mejoras futuras sugeridas âš ï¸

* Configurar Spark como proceso 24/7.
* Permitir creaciÃ³n segura de topics.
* Separar desarrollo de producciÃ³n vÃ­a volÃºmenes persistentes.
* Automatizar ejecuciÃ³n de `spark_stream.py` desde Airflow u otro scheduler.

---

Â¿Listo para escalar esto a producciÃ³n? ğŸš€

No, pero si.

